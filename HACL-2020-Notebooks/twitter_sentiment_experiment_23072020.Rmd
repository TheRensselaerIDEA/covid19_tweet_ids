
```{r}
elasticsearch_host <- "lp01.idea.rpi.edu"
```

```{r setup, include=FALSE}
# Required R package installation:
# These will install packages if they are not already installed
# Set the correct default repository
r = getOption("repos")
r["CRAN"] = "http://cran.rstudio.com"
options(repos = r)

if (!require("ggplot2")) {
  install.packages("ggplot2")
  library(ggplot2)
}

if (!require("knitr")) {
  install.packages("knitr")
  library(knitr)
}

if(!require('dplyr')) {
  install.packages("dplyr")
  library(dplyr)
}

if(!require('stringr')) {
  install.packages("stringr")
  library(stringr)
}

if(!require('Rtsne')) {
  install.packages("Rtsne")
  library(Rtsne)
}

if(!require('stopwords')) {
  install.packages("stopwords")
  library(stopwords)
}

if(!require('plotly')) {
  install.packages("plotly")
  library(plotly)
}

if (!require("kableExtra")) {
  install.packages("kableExtra")
  library(kableExtra)
}

if (!require("vader")) {
  install.packages("vader")
  library(vader)
}

if (!require("NbClust")) {
  install.packages("NbClust")
  library(NbClust)
}

knitr::opts_chunk$set(echo = TRUE)

source("Elasticsearch.R")
```
## Summary

This notebook consists of an experiment using the VADER algorithm to perform sentiment analysis on the Tweets and also cluster them based on sentiment. 

VADER is lexicon based and especially made for the contexts of social media websites like Twitter and Facerbook that assigns a sentence a compound sentiment score in the range [-1,1], -1 being very negative, 0 neutral and 1 very positive. VADER is also able to detect valence, which is the usage of certain words to shift the polarity of sentiment in text without a change in word usage. For example, saying "I like pizza" conveys positive sentiment by the word "like", while saying "I don't like pizza" conveys negative sentiment by the addition of "don't", event though the positive word "like" is still kept.

We query 10,000 tweets from the COVID Events Dataset with the text filter `"cure prevent"` in order to get tweets somewhat realated to COVID-19 cures and prevention methods. We then use VADER to compute the sentiment score of each tweet and proceed to cluster them based on their embeddings using k-means.
Afterwards, we take each cluster separetly, multiply the embedding vector by the sentiment score and perform subclustering on these rescaled tweets.

While the meaning, subtopic or discussion of each subcluster are still yet unclear, we note that the sentiment scaled embedding clusters are very clear distinct when visualized in 2D using t-SNE. The R implementation of VADER is very simple to use and works well with dataframes without the need of extra preprocessing, thus, it could be worthwhile to run it on all available tweets in the backend in order to simplify future temporal and spatial analysis.

### Configure the search parameters here - set date range and semantic phrase:

**THIS SECTION IS IDENTICAL TO THE ORIGINAL NOTEBOOK**

Note: large date ranges can take some time to process on initial search due to the sheer volume of data we have collected. Subsequent searches using the same date range should run quickly due to Elasticsearch caching.

```{r}
# query start date/time (inclusive)
rangestart <- "2020-01-01 00:00:00"

# query end date/time (exclusive)
rangeend <- "2020-08-01 00:00:00"

# text filter restricts results to only those containing words, phrases, or meeting a boolean condition. This query syntax is very flexible and supports a wide variety of filter scenarios:
# words: text_filter <- "cdc nih who"  ...contains "cdc" or "nih" or "who"
# phrase: text_filter <- '"vitamin c"' ...contains exact phrase "vitamin c"
# boolean condition: <- '(cdc nih who) +"vitamin c"' ...contains ("cdc" or "nih" or "who") and exact phrase "vitamin c"
#full specification here: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html
text_filter <- "cure prevent"

# query semantic similarity phrase (choose one of these examples or enter your own)
#semantic_phrase <- "Elementary school students are not coping well with distance learning."
#semantic_phrase <- "How do you stay at home when you are homeless?"
#semantic_phrase <- "My wedding has been postponed due to the coronavirus."
#semantic_phrase <- "I lost my job because of COVID-19. How am I going to be able to make rent?"
#semantic_phrase <- "I am diabetic and out of work because of coronavirus. I am worried I won't be able to get insulin without insurance."
#semantic_phrase <- "There is going to be a COVID-19 baby boom..."
#semantic_phrase <- "Vitamin"
semantic_phrase <- ""

# return results in chronological order or as a random sample within the range
# (ignored if semantic_phrase is not blank)
random_sample <- FALSE
# number of results to return (max 10,000)
resultsize <- 10000

####TEMPORARY SETTINGS####
# number of subclusters per high level cluster (temporary until automatic selection implemented)
cluster.k <- 3
# show/hide extra info (temporary until tabs are implemented)
show_original_subcluster_plots <- FALSE
show_regrouped_subcluster_plots <- TRUE
show_word_freqs <- FALSE
show_center_nn <- FALSE
```

```{r, echo=FALSE}
###############################################################################
# Get the tweets from Elasticsearch using the search parameters defined above
###############################################################################

results <- do_search(indexname="covidevents-data", 
                     rangestart=rangestart,
                     rangeend=rangeend,
                     text_filter=text_filter,
                     semantic_phrase=semantic_phrase,
                     must_have_embedding=TRUE,
                     random_sample=random_sample,
                     resultsize=resultsize,
                     resultfields='"user.screen_name", "user.verified", "user.location", "place.full_name", "place.country", "text", "full_text", "extended_tweet.full_text", "embedding.use_large.primary", "dataset_file", "dataset_entry.annotation.part1.Response", "dataset_entry.annotation.part2-opinion.Response"',
                     elasticsearch_host="lp01.idea.rpi.edu",
                     elasticsearch_path="elasticsearch",
                     elasticsearch_port=443,
                     elasticsearch_schema="https")

# this dataframe contains the tweet text and other metadata
tweet.vectors.df <- results$df[,c("full_text", "user_screen_name", "user_verified", "user_location", "place.country", "place.full_name", "dataset_file", "dataset_entry.annotation.part1.Response", "dataset_entry.annotation.part2-opinion.Response")]

# this matrix contains the embedding vectors for every tweet in tweet.vectors.df
tweet.vectors.matrix <- t(simplify2array(results$df[,"embedding.use_large.primary"]))
```

```{r, echo=FALSE}
###############################################################################
# Clean the tweet and user location text, and set up tweet.vectors.df 
# the way we want it by consolidating the location field and computing
# location type
###############################################################################

tweet.vectors.df$user_location <- ifelse(is.na(tweet.vectors.df$place.full_name), tweet.vectors.df$user_location, paste(tweet.vectors.df$place.full_name, tweet.vectors.df$place.country, sep=", "))
tweet.vectors.df$user_location[is.na(tweet.vectors.df$user_location)] <- ""
tweet.vectors.df$user_location_type <- ifelse(is.na(tweet.vectors.df$place.full_name), "User", "Place")
tweet.vectors.df$class <- sapply(tweet.vectors.df$dataset_file, function(d) sub(".jsonl", "", d))
colnames(tweet.vectors.df)[colnames(tweet.vectors.df) == "dataset_entry.annotation.part1.Response"] <- "is_specific_event"
colnames(tweet.vectors.df)[colnames(tweet.vectors.df) == "dataset_entry.annotation.part2-opinion.Response"] <- "opinion"
tweet.vectors.df <- tweet.vectors.df[, c("full_text", "user_screen_name", "user_verified", "user_location", "user_location_type", "class", "is_specific_event", "opinion")]

clean_text <- function(text, for_freq=FALSE) {
  text <- str_replace_all(text, "[\\s]+", " ")
  text <- str_replace_all(text, "http\\S+", "")
  if (isTRUE(for_freq)) {
    text <- tolower(text)
    text <- str_replace_all(text, "’", "'")
    text <- str_replace_all(text, "_", "-")
    text <- str_replace_all(text, "[^a-z1-9 ']", "")
  } else {
    text <- str_replace_all(text, "[^a-zA-Z1-9 `~!@#$%^&*()-_=+\\[\\];:'\",./?’]", "")
  }
  text <- str_replace_all(text, " +", " ")
  text <- trimws(text)
}
tweet.vectors.df$full_text <- sapply(tweet.vectors.df$full_text, clean_text)
tweet.vectors.df$user_location <- sapply(tweet.vectors.df$user_location, clean_text)
```

```{r, echo=FALSE}
##UNCOMMENT TO GENERATE ELBOW PLOT
# 
# wssplot <- function(data, fc=1, nc=40, seed=20){
#   wss <- data.frame(k=fc:nc, withinss=c(0))
#   for (i in fc:nc){
#     set.seed(seed)
#     wss[i-fc+1,2] <- sum(kmeans(data, centers=i, iter.max=30)$withinss)}
# 
#   ggplot(data=wss,aes(x=k,y=withinss)) +
#     geom_line() +
#     ggtitle("Quality (within sums of squares) of k-means by choice of k")
# }
# wssplot(tweet.vectors.matrix)
```

### Using VADER and Clustering

Next, we add a new column to the dataframe of tweets consisting of the VADER compound sentiment score.

```{r}
####################################################
# Compute and attach tweet sentiment to each tweet
####################################################

tweet.vectors.df$sentiment <- c(0)
tweet.vectors.df$sentiment <- vader_df(tweet.vectors.df$full_text)[,"compound"]

print(tweet.vectors.df)
```

We then cluster the tweet embedding using k-means with 4 clusters. This specific number of clusters was achieved using the `NbClust` package and function, which computes several metrics for a given range of clusters and returns the best number as given by mojority rule. It runs very slowly when computing all metrics, but as Dr. Erickson mentioned, there might be a way to parallelize this computation, which would be worthwhile in the context of an app.

```{r}
###############################################################################
# Run K-means on all the tweet embedding vectors
###############################################################################

# Run NbClust to find an optimal number of clusters, takes a while
#k <- NbClust(data = tweet.vectors.matrix, min.nc = 2, max.nc = 20, method = "kmeans") 
k <- 4 # Use 4 or any other integer for quick trials

set.seed(300)
km <- kmeans(tweet.vectors.matrix, centers=k, iter.max=30)

tweet.vectors.df$vector_type <- factor("tweet", levels=c("tweet", "cluster_center", "subcluster_center"))
tweet.vectors.df$cluster <- as.factor(km$cluster)

#append cluster centers to dataset for visualization
centers.df <- data.frame(full_text=paste("Cluster (", rownames(km$centers), ") Center", sep=""),
                         user_screen_name="[N/A]",
                         user_verified="[N/A]",
                         user_location="[N/A]",
                         user_location_type = "[N/A]",
                         class = "[N/A]",
                         is_specific_event = "[N/A]",
                         opinion = "[N/A]",
                         vector_type = "cluster_center",
                         cluster=as.factor(rownames(km$centers)),
                         sentiment=0.0)
tweet.vectors.df <- rbind(tweet.vectors.df, centers.df)
tweet.vectors.matrix <- rbind(tweet.vectors.matrix, km$centers)
```

Afterwards, we consider each cluster, multiply the embeddings by their sentiment score and perform subclustering on these scaled vectors using k-means. Here, we utilize 3 clusters so as to convey tweets with positive, neutral and negative sentiment, which is a decision not made on the basis of elbow plots or `NbClust` for the sake of time. However, once cluster number selection is automated, it'd be only natural to appy it here too.

```{r}
###########################################################
# Obtain compund sentiment of clustered tweets using VADER
# Then perform subclustering based on tweet vectors scaled
# by sentiment score
###########################################################
tweet.vectors.df$subcluster <- c(0)

for (i in 1:k) {
  set.seed(500)
  cluster.tweets <- tweet.vectors.df[tweet.vectors.df$cluster == i,]
  cluster.matrix <- tweet.vectors.matrix[tweet.vectors.df$cluster == i,]
  cluster.matrix.sentiment.applied <- sweep(cluster.matrix, MARGIN = 1, cluster.tweets$sentiment, `*`)
  cluster.km <- kmeans(cluster.matrix.sentiment.applied, centers=cluster.k, iter.max=30)
  tweet.vectors.df[tweet.vectors.df$cluster == i, "subcluster"] <- cluster.km$cluster
 
 #append subcluster centers to dataset for visualization
   centers.df <- data.frame(full_text=paste("Subcluster (", rownames(cluster.km$centers), ") Center", sep=""),
                           user_screen_name="[N/A]",
                           user_verified="[N/A]",
                           user_location="[N/A]",
                           user_location_type = "[N/A]",
                           class = "[N/A]",
                           is_specific_event = "[N/A]",
                           opinion = "[N/A]",
                           vector_type = "subcluster_center",
                           cluster=as.factor(i),
                           sentiment=0.0,
                           subcluster=rownames(cluster.km$centers))
   tweet.vectors.df <- rbind(tweet.vectors.df, centers.df)
   tweet.vectors.matrix <- rbind(tweet.vectors.matrix, cluster.km$centers)
}
tweet.vectors.df$subcluster <- as.factor(tweet.vectors.df$subcluster)

```

```{r, echo=FALSE}
###############################################################################
# Run K-means again on all the tweet embedding vectors in each cluster
# to create subclusters of tweets
###############################################################################

# tweet.vectors.df$subcluster <- c(0)
# 
# for (i in 1:k){
#  print(paste("Subclustering cluster", i, "..."))
#  cluster.matrix <- tweet.vectors.matrix[tweet.vectors.df$cluster == i,]
#  set.seed(500)
#  cluster.km <- kmeans(cluster.matrix, centers=cluster.k, iter.max=30)
#  tweet.vectors.df[tweet.vectors.df$cluster == i, "subcluster"] <- cluster.km$cluster
#  
#  #append subcluster centers to dataset for visualization
#  centers.df <- data.frame(full_text=paste("Subcluster (", rownames(cluster.km$centers), ") Center", sep=""),
#                          user_screen_name="[N/A]",
#                          user_verified="[N/A]",
#                          user_location="[N/A]",
#                          user_location_type = "[N/A]",
#                          class = "[N/A]",
#                          is_specific_event = "[N/A]",
#                          opinion = "[N/A]",
#                          vector_type = "subcluster_center",
#                          cluster=as.factor(i),
#                          subcluster=rownames(cluster.km$centers))
#  tweet.vectors.df <- rbind(tweet.vectors.df, centers.df)
#  tweet.vectors.matrix <- rbind(tweet.vectors.matrix, cluster.km$centers)
# }
# tweet.vectors.df$subcluster <- as.factor(tweet.vectors.df$subcluster)
```

```{r echo=FALSE}
##UNCOMMENT TO OUTPUT FILES FOR TENSORBOARD

# tweet.vectors.df$cluster_str <- paste("(", tweet.vectors.df$cluster, ")", sep="")
# tweet.vectors.df$subcluster_str <- paste("(", tweet.vectors.df$subcluster, ")", sep="")
# 
# metadata_cols <- setdiff(colnames(tweet.vectors.df), c("cluster", "subcluster"))
# write.table(tweet.vectors.df[,metadata_cols], "clustered_tweet_labels.tsv", sep='\t', row.names = FALSE)
# write.table(tweet.vectors.matrix, "clustered_tweet_vectors.tsv", sep='\t', row.names = FALSE, col.names = FALSE)
# read.table("clustered_tweet_labels.tsv", header=TRUE)
# read.table("clustered_tweet_vectors.tsv", header=TRUE)
```

```{r, echo=FALSE}
###############################################################################
# Compute labels for each cluster and subcluster based on word frequency
# and identify the nearest neighbors to each cluster and subcluster center
###############################################################################

stop_words <- stopwords("en", source="snowball")
stop_words <- union(stop_words, stopwords("en", source="nltk"))
stop_words <- union(stop_words, stopwords("en", source="smart"))
stop_words <- union(stop_words, stopwords("en", source="marimo"))
stop_words <- union(stop_words, c(",", ".", "!", "-", "?", "&amp;", "amp"))

get_word_freqs <- function(full_text) {
  word_freqs <- table(unlist(strsplit(clean_text(full_text, TRUE), " ")))
  word_freqs <- cbind.data.frame(names(word_freqs), as.integer(word_freqs))
  colnames(word_freqs) <- c("word", "count")
  word_freqs <- word_freqs[!(word_freqs$word %in% stop_words),]
  word_freqs <- word_freqs[order(word_freqs$count, decreasing=TRUE),]
}

get_label <- function(word_freqs, exclude_from_labels=NULL, top_k=3) {
  words <- as.character(word_freqs$word)
  exclude_words <- NULL
  if (!is.null(exclude_from_labels)) {
    exclude_words <- unique(unlist(lapply(strsplit(exclude_from_labels, "/"), trimws)))
  }
  label <- paste(setdiff(words, exclude_words)[1:top_k], collapse=" / ")
}

get_nearest_center <- function(df, mtx, center) {
  df$center_cosine_similarity <- apply(mtx, 1, function(v) (v %*% center)/(norm(v, type="2")*norm(center, type="2")))
  nearest_center <- df[order(df$center_cosine_similarity, decreasing=TRUE),]
  nearest_center <- nearest_center[nearest_center$vector_type=="tweet", c("center_cosine_similarity", "full_text", "user_location")]
}

master.word_freqs <- get_word_freqs(tweet.vectors.df$full_text)
master.label <- get_label(master.word_freqs, top_k=6)

clusters <- list()
for (i in 1:k) {
  cluster.df <- tweet.vectors.df[tweet.vectors.df$cluster == i,]
  cluster.matrix <- tweet.vectors.matrix[tweet.vectors.df$cluster == i,]
    
  cluster.word_freqs <- get_word_freqs(cluster.df$full_text)
  cluster.label <- get_label(cluster.word_freqs, master.label)
  cluster.center <- cluster.matrix[cluster.df$vector_type=="cluster_center",]
  cluster.nearest_center <- get_nearest_center(cluster.df, cluster.matrix, cluster.center)
  
  cluster.subclusters <- list()
  for (j in 1:cluster.k) {
    subcluster.df <- cluster.df[cluster.df$subcluster == j,]
    subcluster.matrix <- cluster.matrix[cluster.df$subcluster == j,]
    
    subcluster.word_freqs <- get_word_freqs(subcluster.df$full_text)
    subcluster.label <- get_label(subcluster.word_freqs, c(master.label, cluster.label))
    subcluster.center <- subcluster.matrix[subcluster.df$vector_type=="subcluster_center",]
    subcluster.nearest_center <- get_nearest_center(subcluster.df, subcluster.matrix, subcluster.center)
    
    cluster.subclusters[[j]] <- list(word_freqs=subcluster.word_freqs, label=subcluster.label, nearest_center=subcluster.nearest_center)
  }
  
  clusters[[i]] <- list(word_freqs=cluster.word_freqs, label=cluster.label, nearest_center=cluster.nearest_center, subclusters=cluster.subclusters)
}

```

We now visualize the clusters and subclusters using t-SNE. The cluster plot shows the original embeddings, while the subcluster plots show the rescaled embeddings. We can clearly see how each subcluster is well defined. 

```{r, echo=FALSE}
###############################################################################
# Run T-SNE on all the tweets and then again on each *sentiment scaled* cluster 
# to get plot coordinates for each tweet. We output a master plot with all 
# clusters and a cluster plot with all subclusters for each cluster.
###############################################################################

set.seed(700)
tsne <- Rtsne(tweet.vectors.matrix, dims=2, perplexity=25, max_iter=750, check_duplicates=FALSE)
tsne.plot <- cbind(tsne$Y, tweet.vectors.df)
colnames(tsne.plot)[1:2] <- c("X", "Y")
tsne.plot$full_text <- sapply(tsne.plot$full_text, function(t) paste(strwrap(t ,width=60), collapse="<br>"))
tsne.plot$cluster.label <- sapply(tsne.plot$cluster, function(c) clusters[[c]]$label)

taglist <- htmltools::tagList()

#Master high level plot
fig <- plot_ly(tsne.plot, x=~X, y=~Y, 
               text=~paste("Cluster:", cluster, "<br>Class:", class, "<br>IsSpecificEvent:", is_specific_event, "<br>Opinion:", opinion, "<br>Text:", full_text), 
               color=~cluster.label, type="scatter", mode="markers")
fig <- fig %>% layout(title=paste("Master Plot:", master.label, "(high level clusters)"), 
                        yaxis=list(zeroline=FALSE), 
                        xaxis=list(zeroline=FALSE))
fig <- fig %>% toWebGL()
taglist[[1]] <- fig

#Cluster plots
plot_index <- 2
for (i in 1:k) {
  print(paste("Plotting cluster", i, "..."))
  cluster.matrix <- sweep(tweet.vectors.matrix[tsne.plot$cluster == i,], MARGIN = 1, tweet.vectors.df$sentiment[tsne.plot$cluster == i], `*`)
  
  set.seed(900)
  cluster.tsne <- Rtsne(cluster.matrix, dims=2, perplexity=12, max_iter=500, check_duplicates=FALSE)
  cluster.tsne.plot <- cbind(cluster.tsne$Y, tsne.plot[tsne.plot$cluster == i,])
  colnames(cluster.tsne.plot)[1:2] <- c("cluster.X", "cluster.Y")
  cluster.tsne.plot$subcluster.label <- sapply(cluster.tsne.plot$subcluster, function(c) clusters[[i]]$subclusters[[c]]$label)
  
  #Cluster plot with original positions
  if (isTRUE(show_original_subcluster_plots)) {
    fig <- plot_ly(cluster.tsne.plot, x=~X, y=~Y, 
                   text=~paste("Subcluster:", subcluster, "<br>Class:", class, "<br>IsSpecificEvent:", is_specific_event, "<br>Opinion:", opinion, "<br>Text:", full_text), 
                   color=~subcluster.label, type="scatter", mode="markers")
    fig <- fig %>% layout(title=paste('Cluster ', i, ": ", clusters[[i]]$label, " (as positioned in master plot)", sep=""), 
                          yaxis=list(zeroline=FALSE), 
                          xaxis=list(zeroline=FALSE))
    #fig <- fig %>% toWebGL()
    taglist[[plot_index]] <- fig
    plot_index <- plot_index + 1
  }
  
  #Cluster plot with regrouped positions by subcluster
  if (isTRUE(show_regrouped_subcluster_plots)) {
    fig <- plot_ly(cluster.tsne.plot, x=~cluster.X, y=~cluster.Y, 
                   text=~paste("Subcluster:", subcluster, "<br>Class:", class, "<br>IsSpecificEvent:", is_specific_event, "<br>Opinion:", opinion, "<br>Text:", full_text), 
                   color=~subcluster.label, type="scatter", mode="markers")
    fig <- fig %>% layout(title=paste('Cluster ', i, ": ", clusters[[i]]$label, " (regrouped by subcluster)", sep=""), 
                          yaxis=list(zeroline=FALSE), 
                          xaxis=list(zeroline=FALSE))
    #fig <- fig %>% toWebGL()
    taglist[[plot_index]] <- fig
    plot_index <- plot_index + 1
  }
  
  # Print cluster word frequencies
  if (isTRUE(show_word_freqs)) {
    taglist[[plot_index]] <- htmltools::HTML(kable(clusters[[i]]$word_freqs[1:5,], caption=paste("Cluster", i, "word frequencies")) %>% kable_styling())
    plot_index <- plot_index + 1
  }
  
  # Print nearest neighbors of cluster center
  if (isTRUE(show_center_nn)) {
    taglist[[plot_index]] <- htmltools::HTML(kable(clusters[[i]]$nearest_center[1:5,], caption=paste("Cluster", i, "nearest neighbors to center")) %>% kable_styling())
    plot_index <- plot_index + 1
  }
  
  for (j in 1:cluster.k) {
    # Print subcluster word frequencies
    if (isTRUE(show_word_freqs)) {
      taglist[[plot_index]] <- htmltools::HTML(kable(clusters[[i]]$subclusters[[j]]$word_freqs[1:5,], caption=paste("Subcluster", j, "word frequencies")) %>% kable_styling())
      plot_index <- plot_index + 1
    }
    
    # Print nearest neighbors of subcluster center
    if (isTRUE(show_center_nn)) {
      taglist[[plot_index]] <- htmltools::HTML(kable(clusters[[i]]$subclusters[[j]]$nearest_center[1:5,], caption=paste("Subcluster", j, "nearest neighbors to center")) %>% kable_styling())
      plot_index <- plot_index + 1
    }
  }
}

taglist
```

While this experiment shows some potential in using sentiment in clustering there are still many questions left unanswered:
 
* What's the sentiment distribution in each cluster? Is it unimodal or bimodal?
* We used sentiment in subclustering, but could we use it before clustering or after subclustering?
* What does each subcluster mean? Are the subclusters different subtopics with their own sentiment profile or are they different takes on the cluster topic?
* All the temporal, spatial and reactionary questions still floating around.
